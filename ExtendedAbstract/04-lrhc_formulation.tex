\section{A hybrid approach: \textnormal{\textit{Learning-Based Receding Horizon Control with Reinforcement Learning}}}
Most of the currently employed control tools and pipelines for locomotion rely either on online ``model-based'' controllers~\cite{modern_mpc:grandia2023perceptive,web::atlas_grip_boston_dyn} or ``model-free'' learned policies (often RL-based and trained offline)~\cite{mpc_learn:aswani2012provably, mpc_learn:terzi2018learning, mpc_learn:soloperto2018learning, rl:schneider2023learning, rl:miki2024learning,mpc_learn:berkenkamp2016safe,mpc_learn:marco2016automatic,mpc_learn:brunner2015stabilizing,mpc_learn:rosolia2019learning,mpc_learn:englert2017inverse,mpc_learn:koller2018learning,mpc_learn:wabersich2021probabilistic,mpc_learn:gillulay2011guaranteed,mpc_learn:wabersich2018safe,mpc_learn:berkenkamp2017safe}, with few exceptions~\cite{hybrid_rl_to:Jenelten_2024}.
In the past years there have been several attempts at combining learning-based methods and receding horizon controllers, e.g.~\cite{mpc_learn:tsounis2020deepgait,mpc_learn:gangapurwala2021real}. Specifically, the following main approaches can be identified~\cite{mpc_learn:hewing2020learning}:
\begin{itemize}
	\item[1)] \textit{Model augmentation}: integration of learned models into RHC controllers to improve prediction accuracy and control performance~\cite{mpc_learn:aswani2012provably,mpc_learn:terzi2018learning,mpc_learn:soloperto2018learning}.
	\item[2)] \textit{Adaptive tuning} and \textit{parameter optimization}: RHC parameters tuning (e.g. weights, costs, constraints), based on real-time data~\cite{mpc_learn:berkenkamp2016safe,mpc_learn:marco2016automatic,mpc_learn:brunner2015stabilizing,mpc_learn:englert2017inverse,mpc_learn:rosolia2019learning,mpc_learn:romero2023actor}.
	\item[3)] \textit{Safety}: a learned-policy is coupled with a RHC controller, which in this context takes the role of a \textit{safety filter}~\cite{mpc_learn:koller2018learning,mpc_learn:wabersich2021probabilistic,mpc_learn:gillulay2011guaranteed,mpc_learn:wabersich2018safe,mpc_learn:berkenkamp2017safe}.
\end{itemize}
Our approach to RL-based RHC, which is synthetically depicted in Fig.~\ref{fig:lrhc_arch}, can be framed as a hybrid between 1) and 3) and it is to some extent complementary to what was done in~\cite{mpc_learn:hewing2020learning}, where a RHC is used to rollout reference motions and footstep plans during the training of a RL tracking policy. Instead of using the RHC controller just for training, we actually aim at hierarchically coupling it with a higher level agent during both training and real-world deployment. This allows to tackle problems which are non-trivial at the RHC level (like contact phase selection), while also exploiting the robustness and flexibility of the agent and the safety guarantees of the RHC controller. 
This approach, however, entails several challenges, particularly from a practical point of view (integration, computational complexity, generalization, reward formulation), which indeed make the required implementation effort non-negligible. 