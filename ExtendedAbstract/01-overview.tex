\section{A Hystorical Overview: \textnormal{\textit{from Markov Decision Processes and Dynamic Programming to modern Receding Horizon Control and Reinforcement Learning}}}
The foundation of \textbf{Markov Decision Processes} (MDPs) can be traced back to the work of Andrey Markov, a Russian mathematician, who developed the theory of Markov chains in the early 20th century
MDPs. In the late 1950s and early 1960s, Richard Bellman, along with his colleague Howard Dreyfus, introduced MDPs as a mathematical framework for sequential decision-making under uncertainty~\cite{rl:bellman1957markovian}.
MDPs consist of a set of state $S$, representing the possible configurations or conditions of the system being modeled. At any given time, the system is in one of these states. For each state in the MDP, there is a set of possible actions $A$ that the decision-maker can choose from. Upon taking an action $a\in\,A$ in a particular state $s\in\,S$, the system transitions to a new state $s'\in\,S$ according to a probability distribution $P(s' \mid s, a)$. These transition probabilities capture the stochastic nature of the environment and its dynamics. Associated with each $\{s,\,a\}$ pair is an immediate reward $r(s,\,a)$, expression of the immediate benefit or cost incurred by the decision-maker upon taking that action in that state. The objective in MDPs is to find an optimal policy $\pi(a \vert s)$ that maximizes the cumulative reward obtained by the decision-maker over time. This involves balancing the trade-off between immediate rewards and long-term benefits, taking into account the probabilistic nature of the environment and the dynamics of the system.
In the late 1950s, Richard Bellman introduced the so-called \textbf{Bellman equation}:\\
\textit{Let $\pi^*$ be an optimal policy for a Markov Decision Process (MDP) with state space $S$, action space $A$, state transition probabilities $P(s' \mid s, a)$, and immediate rewards $r(s, a)$. Then, for any state $s$ in the state space $S$, the following equation holds:
	\[
	V^*(s) = \max_{a \in A} \left[ r(s, a) + \gamma \sum_{s' \in S} P(s' \mid s, a) V^*(s') \right]
	\]
	where:
	\begin{itemize}
		\item $V^*(s)$ is the optimal value function for state $s$,
		\item $r(s, a)$ is the immediate reward received upon taking action $a$ in state $s$,
		\item $P(s' \mid s, a)$ is the probability of transitioning to state $s'$ after taking action $a$ in state $s$,
		\item $\gamma$ is the discount factor representing the importance of future rewards, and
		\item $\max_{a \in A} [\cdot]$ denotes the maximum over all possible actions in state $s$.
\end{itemize}}
Building upon the formulation of MDPs and the above Bellman Equation, Richard Bellman and other researchers developed the theory of Dynamic Programming (DP) as a systematic method for solving optimization problems by breaking them down into simpler subproblems~\cite{rl:bellman1960dynamic}.

\cite{psyc:skinner2019behavior}

\cite{rl:rumelhart1986learning}
\cite{rl:kakade2001natural}
\cite{rl:peters2005natural}
\cite{rl:degris2012off}
\cite{rl:schulman2015trust}
\cite{rl:schulman2017proximal}
\cite{rl:pardo2018time}
\cite{rl:haarnoja2018soft}
\cite{rl:makoviychuk2021isaac}
\cite{rl:rudin2022learning}
\cite{rl:schneider2023learning}
\cite{rl:mujocoaccelereted2023}
\cite{rl:miki2024learning}

\cite{frameworks::horizon_to}

\cite{web::lrhc_boston_dyn}
\cite{frameworks:mittal2023orbit}
\cite{frameworks:howell2022}
\cite{mpc_learn:hewing2020learning}