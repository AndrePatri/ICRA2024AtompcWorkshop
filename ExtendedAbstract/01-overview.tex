\section{A Brief Hystorical Overview: \textnormal{\textit{from Markov Decision Processes and Dynamic Programming to modern Receding Horizon Control and Reinforcement Learning}}}
\begin{figure*}[t]
	\centering
	\vspace{0.1cm}
	\includegraphics[width=0.9\textwidth]{imgs/learning_based_rhc.pdf}
	\caption{Our take on Learning-based Receding Horizon Control: a RHC controller is hierarchically coupled with a higher-level RL agent during both training and real-world deployment. The RL agent has control over key RHC run-time parameters like contact phases and twist commands and can monitor its internal state (costs, constraint violations). The agent learns to exploit the underlying RHC controller to perform the tracking of user-specified high-level task references.}
	\label{fig:lrhc_arch}
	\vspace{-0.3cm}
\end{figure*}
State-of-the-art of locomotion and manipulation pipelines have been shown to be capable of remarkable performance and robustness~\cite{rl:schneider2023learning,rl:miki2024learning,web::atlas_grip_boston_dyn,web::lrhc_boston_dyn}. These results stand on the shoulders of more than seventy years of research in robotics, control and learning, starting from the very first industrial automated robot \textit{Unimate} in the 1950s~\cite{origins:xu2018fourth}, Richard Bellman's pioneering work in the late 1950s and early 1960s on \textit{Markov Decision Processes} (MDPs)~\cite{rl:bellman1957markovian} and \textit{Dynamic Programming}~\cite{rl:bellman1960dynamic} (DP), and the establishment of \textit{Machine Learning} (ML) as consolidated field of study.

MDPs are a mathematical framework used to model sequential decision-making and are described by a set of states $S$ and a set of possible actions $A$ that the decision-maker can choose from. Upon taking an action $a\in\,A$ in a particular state $s\in\,S$, the system transitions to a new state $s'\in\,S$, with associated immediate reward $r(s,\,a,\,s')$, according to a probability distribution $p(s'\mid s, a)$. Reinforcement Learning (RL) methods aim at finding an optimal policy $\pi^{*}(a\,\vert\,s)$ that maximizes the cumulative (discounted) reward $R_t = \sum_{k=0}^{n-1} \gamma^k \cdot r_{t+k}$ obtained by the decision-maker over a time horizon.

The formulation of MPDs, in conjunction with the introduction of the \textit{Bellman equation}~\cite{rl:bellman1960dynamic}, the \textit{value functions} and \textit{Value Iteration} algorithm, laid the foundations of DP as a systematic method for solving sequential-decision problems by breaking them down into simpler sub-problems~\cite{rl:bellman1960dynamic}. The development of \textit{Policy Iteration}~\cite{rl:howard1960dynamic}, \textit{TD-learning}~\cite{rl:barto1983neuronlike}, \textit{Q-learning}~\cite{rl:watkins1989learning}, the increased popularization of back-propagation~\cite{rl:rumelhart1986learning} as a way of training powerful neural function approximators and the ever-increasing computational resources progressively paved the way to today's most successful and employed on-policy and off-policy RL algorithms PPO~\cite{rl:schulman2017proximal} and SAC~\cite{rl:haarnoja2018soft}, respectively. 
%Some of these ancestors include, for instance, the \textit{Natural Policy Gradient}~\cite{rl:kakade2001natural}, \textit{Deep Q-Networks} (DQN)~\cite{rl:mnih2015human},~\textit{Deep Deterministic Policy Gradient} (DDPG)~\cite{rl:lillicrap2015continuous} and \textit{Trust Region Policy Optimization}~\cite{rl:schulman2015trust} algorithms.

In an analogous way, DP principles served as a foundational basis for the evolution of modern RHC~\cite{modern_mpc:grandia2023perceptive}, which iteratively solves a finite-horizon optimal control problem over short time intervals, considering system dynamics and constraints. Over the years, many algorithms for the solution of receding-horizon nonlinear optimization problems have been developed, and several of them are directly tied to the continuous-time dynamics declination of DP, namely \textit{Differential Dynamic Programming} (DDP)~\cite{rhc:jacobson1970differential,rhc:todorov2005generalized,rhc:diehl2009efficient,rhc:tassa2012synthesis}.