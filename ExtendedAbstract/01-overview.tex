\section{A Brief Hystorical Overview: \textnormal{\textit{from Markov Decision Processes and Dynamic Programming to modern Receding Horizon Control and Reinforcement Learning}}}
\begin{figure*}[t]
	\centering
	\vspace{0.1cm}
	\includegraphics[width=0.9\textwidth]{imgs/learning_based_rhc.pdf}
	\caption{Our take on Learning-based Receding Horizon Control: a RHC controller is hierarchically coupled with a higher-level RL agent during both training and real-world deployment. The RL agent has control over key RHC run-time parameters like contact phases and twist commands and can monitor its internal state (costs, constraint violations). The agent learns to exploit the underlying RHC controller to perform the tracking of user-specified high-level task references.}
	\label{fig:lrhc_arch}
\end{figure*}
State-of-the-art of locomotion and manipulation pipelines have been shown to be capable of remarkable performance and robustness~\cite{rl:schneider2023learning,rl:miki2024learning,web::atlas_grip_boston_dyn,web::lrhc_boston_dyn}. These results stand on the shoulders of more than seventy years of research in robotics, control and learning, starting from the very first industrial automated robot \textit{Unimate} in the 1950s~\cite{origins:xu2018fourth}, Richard Bellman's pioneering work in the late 1950s and early 1960s on \textbf{Markov Decision Processes} (MDPs)~\cite{rl:bellman1957markovian} and \textbf{Dynamic Programming}~\cite{rl:bellman1960dynamic} (DP) and the birth of \textbf{Artificial Intelligence} (AI) as an established field of study thanks to the contributions of researchers like Alan Turing, John McCarthy, Marvin Minsky and Claude Shannon.
Specifically, the introduction of the so-called \textit{Bellman equation}~\cite{rl:bellman1960dynamic} for the \textit{Value Function} in conjunction with the formulation of MDPs, laid the foundations of DP as a systematic method for solving sequential decision-making problems by breaking them down into simpler sub-problems~\cite{rl:bellman1960dynamic}. Later on, the development of \textit{Policy Iteration}~\cite{rl:howard1960dynamic}, \textit{TD-learning}~\cite{rl:barto1983neuronlike}, \textit{Q-learning}~\cite{rl:watkins1989learning}, the increased popularization of back-propagation~\cite{rl:rumelhart1986learning} as a way of training powerful neural function approximators and the ever-increasing computational resources progressively paved the way to the ancestors of today's most successful and employed on-policy and off-policy Reinforcement Learning (RL) algorithms PPO~\cite{rl:schulman2017proximal} and SAC~\cite{rl:haarnoja2018soft}, respectively. Some of these ancestors include, for instance, the \textit{Natural Policy Gradient}~\cite{rl:kakade2001natural}, \textit{Deep Q-Networks} (DQN)~\cite{rl:mnih2015human},~\textit{Deep Deterministic Policy Gradient} (DDPG)~\cite{rl:lillicrap2015continuous} and \textit{Trust Region Policy Optimization}~\cite{rl:schulman2015trust} algorithms.
In an analogous way, DP principles served as a foundational basis for the evolution of modern RHC~\cite{modern_mpc:grandia2023perceptive}.
Just as DP breaks down complex problems into smaller subproblems and iteratively finds optimal solutions by considering future consequences, RHC iteratively solves finite-horizon optimal control problems over shorter time intervals, considering system dynamics and constraints. Over the years, many algorithms for the solution of the classical receding-horizon nonlinear optimization problem have been developed, and several of them are directly tied to the continuous-time dynamics evolution of DP, namely \textit{Differential Dynamic Programming} (DDP)~\cite{rhc:jacobson1970differential,rhc:todorov2005generalized,rhc:diehl2009efficient,rhc:tassa2012synthesis}.