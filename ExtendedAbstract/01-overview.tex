\section{A Hystorical Perspective: \textnormal{\textit{from Markov Decision Processes and Dynamic Programming to modern Receding Horizon Control and Reinforcement Learning}}}
State-of-the-art of locomotion and manipulation pipelines have been shown to be capable of remarkable performance and robustness~\cite{rl:schneider2023learning,rl:miki2024learning,web::atlas_grip_boston_dyn,web::lrhc_boston_dyn}. These results stand on the shoulders of more than seventy years of research in robotics, control and learning, starting from the very first industrial automated robot \textit{Unimate} in the 1950s~\cite{origins:xu2018fourth}, Richard Bellman's pioneering work in the late 1950s and early 1960s on \textbf{Markov Decision Processes} (MDPs)~\cite{rl:bellman1957markovian} and \textbf{Dynamic Programming}~\cite{rl:bellman1960dynamic} (DP) and the birth of \textbf{Artificial Intelligence} (AI) as an established field of study thanks to the contributions of researchers like Alan Turing, John McCarthy, Marvin Minsky (\textit{connectionism}) and Claude Shannon (\textit{information theory}).
MDPs are a mathematical framework used to model sequential decision-making in situations where outcomes are influenced by potentially probabilistic transitions and the actions taken by a decision-maker. MDPs are described by a set of state $S$, representing the possible configurations or conditions of the system being modeled. At any given time, the system is in one of these states. For each state in the MDP, there is a set of possible actions $A$ that the decision-maker can choose from. Upon taking an action $a\in\,A$ in a particular state $s\in\,S$, the system transitions to a new state $s'\in\,S$, with associated immediate reward $r(s,\,a,\,s')$, according to a probability distribution $p(s'\mid s, a)$. The objective in MDPs is to find an optimal policy $\pi^{*}(a\,\vert\,s)$ that maximizes the cumulative (discounted) reward $R_t = \sum_{k=0}^{n-1} \gamma^k \cdot r_{t+k}$ obtained by the decision-maker over a time horizon, possibly infinite.
In the late 1950s, Richard Bellman and other researchers introduced the so-called \textbf{Bellman equation} for the \textit{value function} $V^{\pi}(s) = \mathbb{E}_\pi [R_t|S_t=s]$:
\begin{equation}\label{eq:bellman}
	V^{*}(s) = \sum_{a} \pi(a|s) \sum_{s'} p(s'|s,\,a) \,[r(s,\,a,\,s')\,+\,\gamma\,V^{*}(s')]
\end{equation}
which, in conjunction with the formulation of MDPs laid the foundations of DP as a systematic method for solving sequential decision-making problems by breaking them down into simpler sub-problems~\cite{rl:bellman1960dynamic}. This resulted in the so-called \textit{Value Iteration} algorithm, which allows to solve for the optimal value function by recursive application of equation~\eqref{eq:bellman} and consequently retrieve the optimal policy. Short after, the \textit{Policy Iteration} algorithm~\cite{rl:howard1960dynamic} was proposed as a way to compute the optimal policy by alternating between value function evaluation and policy improvement. Later in the 1980s, \textit{TD-learning} was introduced~\cite{rl:barto1983neuronlike}, with the idea of bootstrapping the learning of value functions for MDPs by
using the current value estimate of temporally successive steps, allowing the development of more computationally efficient learning algorithms. In 1889 Watkins introduced \textit{Q-learning}, which combines TD-learning with DP~\cite{rl:watkins1989learning} in an off-policy (any policy can be used for experience generation) value-based algorithm. Differently from previous approaches, it tries to estimate a ``quality function'' Q(s, a), which indicates the value of being is state s and choosing action a. The increased popularization of back-propagation~\cite{rl:rumelhart1986learning} as a way of training powerful function approximators based on neural networks and the ever-increasing computational resources progressively paved the way to the ancestors of today's most successful and employed on and off-policy RL algorithms PPO~\cite{rl:schulman2017proximal} and SAC~\cite{rl:haarnoja2018soft}, respectively. Some of these ancestors include, for instance, the \textit{Natural Policy Gradient}~\cite{rl:kakade2001natural}, \textit{Deep Quality Networks} (DQN)~\cite{rl:mnih2015human},~\textit{Deep Deterministic Policy Gradient} (DDPG)~\cite{rl:lillicrap2015continuous} and \textit{Trust Region Policy Optimization}~\cite{rl:schulman2015trust} algorithms.

In an analogous way, Dynamic Programming (DP) principles served as a foundational basis for the evolution of modern RHC~\cite{modern_mpc:grandia2023perceptive}.
DP's emphasis on backward induction and the Bellman equation, which form the cornerstone of its problem-solving methodology, inspired the development of RHC's iterative optimization strategy. Just as DP breaks down complex problems into smaller subproblems and iteratively finds optimal solutions by considering future consequences, RHC iteratively solves finite-horizon optimal control problems over shorter time intervals, considering system dynamics and constraints. 

Over the years, many algorithms for the solution of the classical receding-horizon nonlinear optimization program
\begin{equation}\label{eq:opt_prog}
	\begin{aligned}
		& \underset{u_0,\,\ldots,\,u_{N-2};\,x_1,\,\ldots,\,x_{N-1}}{\text{minimize}}
		& & L(x,\,u) \\
		& \text{subject to}
		& & x_{k+1} = f(x_k, u_k),\,\,\,\,\forall\,k\in[0,\,N-2]\\
		& & & x_k \in \mathcal{X},\,\,\,\,\forall\,k\in[0,\,N-1] \\
		& & & u_k \in \mathcal{U},\,\,\,\,\forall\,k\in[0,\,N-2] \\
		& & & x_0 = x_{\text{init}}
	\end{aligned}
\end{equation}
%where
%\begin{itemize}
%	\item System Dynamics: $\dot{x} = f(x, u)$
%	\item Control Inputs: $u$
%	\item State Constraints: $x \in \mathcal{X}$
%	\item Control Constraints: $u \in \mathcal{U}$
%	\item Objective Function: $L(u) = \sum_{k=0}^{N-1} l(x_k, u_k) + L_t(x_N)$
%\end{itemize}
have been developed, and several of them have direct ties with DP and, specifically, \textit{ Differential Dynamic Programming} (DDP)~\cite{rhc:jacobson1970differential,rhc:todorov2005generalized,rhc:diehl2009efficient,rhc:tassa2012synthesis}.